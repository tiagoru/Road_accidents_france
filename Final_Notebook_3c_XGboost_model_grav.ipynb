{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712d9d1c-1eeb-447d-bcfa-f5c3198b7713",
   "metadata": {},
   "source": [
    "# Modelling of France Accidents\n",
    "\n",
    "**Cohort:** mar23_accidents\n",
    "\n",
    "**Author:** Tobias Schulze\n",
    "\n",
    "**Date:** 30 Oktober 2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6df835-aa14-4775-90df-de5512f881c4",
   "metadata": {},
   "source": [
    "## Loading of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22cfc8cd-2dd7-45e2-bd75-df68ef38c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "import time\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d12d86a-94ac-4349-b68f-2b8a40e21ebe",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aaa9044-2e16-4821-a259-0e2b8fd2b259",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/231030_clean_table_for_analysis.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1765\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1763\u001b[0m         new_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index)\n\u001b[0;32m-> 1765\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/core/frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    731\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/core/internals/construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/core/internals/managers.py:2091\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[0;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[0;32m-> 2091\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/core/internals/managers.py:1750\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[0;32m-> 1750\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/core/internals/managers.py:2217\u001b[0m, in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   2215\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[0;32m-> 2217\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2220\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[1;32m   2221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/pandas/core/internals/managers.py:2242\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   2235\u001b[0m new_values: ArrayLike\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   2238\u001b[0m     \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;66;03m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[1;32m   2240\u001b[0m     \u001b[38;5;66;03m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[1;32m   2241\u001b[0m     \u001b[38;5;66;03m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[0;32m-> 2242\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2244\u001b[0m     bvals \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n",
      "File \u001b[0;32m~/miniforge3/envs/mli-project/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/231030_clean_table_for_analysis.csv', low_memory = False, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552176c-9a85-4ead-ad23-1edfb5c1d448",
   "metadata": {},
   "source": [
    "## Data description\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e361f45-f63c-48a3-bc60-777a1d287f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fbf44b-f1ca-4d02-9b27-78827b41985d",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb68a0-55d1-461b-87e2-72833557b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258ad30-b0f4-4b86-9322-808ff955593e",
   "metadata": {},
   "source": [
    "### Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24315f42-0b75-439c-92a1-ed41d3b2c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54ae8d-7ed1-413e-81cf-04079aa3cec0",
   "metadata": {},
   "source": [
    "This data has no missing values, accept the `holiday` variable which contains the name of the holiday or NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee2b4b-2bca-4686-b48f-b0f0d43b4bed",
   "metadata": {},
   "source": [
    "### Drop variables\n",
    "The dataset contains still some variables that represent information represented by other variables or are a finer granulation. The latter might be added later, if the primary variable is relevant.\n",
    "\n",
    "- `holiday`: classifies the holiday, but will be used only, if `is_holiday` is relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967aff4c-b3a7-4ea6-8e48-80a148b62871",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_drop = ['holiday']\n",
    "\n",
    "df.drop(columns = columns_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50abfde-c053-45b5-a270-ae9adc390856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e466b8d-e6d7-432a-bcd8-da9e0d60c3c5",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "### Transform data types\n",
    "#### Date and time variables\n",
    "The date and time variables are maybe important as grouping variables or as contrains for time dependent severity of accidents.\n",
    "\n",
    "_Problem_:\n",
    "\n",
    "Due to the export to `csv`, the `datatime` data time is lost and thus, it is required to assign it again.\n",
    "\n",
    "**Steps:**\n",
    "1. Transform `date` and `hrmn` to `datetime` type.nais` to integer and `date` to `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175109c-ec79-4bf1-970a-7c6203b99538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the hrmn issue:\n",
    "# Transform `date`\n",
    "df = df.assign(date = pd.to_datetime(df['date']))\n",
    "df = df.assign(hrmn = pd.to_datetime(df['hrmn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ce0e6-281d-4d3e-86e7-b8b7f7b768f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df.apply(lambda x: int(x['date'].timestamp()), axis = 1)\n",
    "df['hrmn'] = df.apply(lambda x: int(x['hrmn'].timestamp()), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4df31-4a36-442f-a1d7-ed33b603dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded7a68-a52c-48e3-9276-33f9f55f132d",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "### Correlation matrix\n",
    "To get a first glimpse on the possible contrains in the data, a correlation matrix is plotted.\n",
    "\n",
    "The data variables are dropped to remove redundany with `timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e4fe3-9b10-4523-90ed-d1d69c65e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540e6aa-5575-4184-a603-870a2c680265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop date columns for the correlation matrix\n",
    "columns_drop = ['an', 'mois', 'jour', 'datetime', 'timestamp'] #, 'date']\n",
    "df_select = df.drop(columns_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79448e-c276-43b9-a87e-9d2ea2157253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "df_numeric_scaled = pd.DataFrame(scaler.fit_transform(df_select), columns=df_select.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88a9ea-8230-4d0e-ae55-77a749dfbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "correlation_matrix = df_numeric_scaled.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Generate the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "# Display the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712a786-07c6-4f26-94b5-640f756e2433",
   "metadata": {},
   "source": [
    "The correlation matrix shows some intervariable correlations, but no real importance of variables to predict the target `fatal`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19180e-4918-4d1f-922a-14525d1a1c29",
   "metadata": {},
   "source": [
    "### Preparation of the training and test datasets\n",
    "#### Split data\n",
    "The dataset is split by year slides. The years 2005 to 2019 are selected for the train dataset and the years 2020 and 2021 for the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2bc919-2233-4813-840b-b7b0bd7b582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to train and test data\n",
    "X_train = df.loc[df['an'] < 2020].drop('grav', axis = 1)\n",
    "X_test = df.loc[df['an'] > 2019].drop('grav', axis = 1)\n",
    "\n",
    "y_train = df['grav'].loc[df['an'] < 2020]\n",
    "y_test = df['grav'].loc[df['an'] > 2019]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26625aa3-4aa4-407e-af35-fc4f79e5d944",
   "metadata": {},
   "source": [
    "#### Data scaling\n",
    "Decision trees are not sensitive to different scales and thus scaling is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935884d9-2884-4e18-a40e-2f6d6d4c6a93",
   "metadata": {},
   "source": [
    "### Modelling the data using a XGBoost Classification Model with Tree-based Parzen Estimators optimisation\n",
    "The Tree-based Parzen Estimator (TPE) [1] optimisation combines a Bayesian Sequential Model Based Optimisation (SMBO) and a random search on the hyperparemeter grid[2]. While the random search is a static approach, the SMBO optimises the model using prior runs to determine future points of exploration.\n",
    "\n",
    "The package `hyperopt` [4] is used for optimisation. The hyperparameter optimisation code is written based on ideas in [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899c3a5-71a0-4cc1-8b15-b8f860f89a90",
   "metadata": {},
   "source": [
    "#### The tuning space\n",
    "`hyperopt` uses a specific format of parameters expressions [7]. Hints on the parameter ranges are given in [1,5,7].\n",
    "\n",
    "The `booster` in this first model is `gbtree`. The evaluation metric is `logloss` or `auc`[8].\n",
    "\n",
    "Another classification method is Dropout with Multiple Additive Regression Trees (`dart`)[9], which is not used here.\n",
    "\n",
    "The metric used is `mlogloss` (as a special case of `logloss` for multiclass modelling). The reason is that `logloss` is an absolute measure of the quality of the classification, while `auc` is a simple ranking function [10]. `logloss` is discussed as better performing with imbalanced datasets [11].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2c349-31d2-4b9e-9573-1e69ce36b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tuning space\n",
    "booster = 'gbtree'\n",
    "eval_metric = 'mlogloss'\n",
    "\n",
    "tuning_space={\n",
    "    'eta': hp.uniform('eta', 0, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'gamma': hp.uniform ('gamma', 0, 10),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha', 0, 100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda', 0, 10),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0, 1),\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0, 1),\n",
    "    'colsample_bynode' : hp.uniform('colsample_bynode', 0, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 1000, 1), # default is 100, so we try to check in the one magnitude space\n",
    "    'seed': 123,\n",
    "    'early_stopping_rounds': 10, # stop tuning early to avoid overfitting\n",
    "    'objective': 'binary:logistic',\n",
    "    'subsample': hp.uniform('subsample', 0, 1)\n",
    "    }\n",
    "\n",
    "tuning_space.update({'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 2)}) # helps in auc\n",
    "tuning_space.update({'max_delta_step': hp.uniform('max_delta_step', 0, 2)}) # helps in auc\n",
    "tuning_space.update({'eval_metric': eval_metric})\n",
    "tuning_space.update({'booster': booster})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b7c6d-e438-43cc-8ec7-2adb7f2e57aa",
   "metadata": {},
   "source": [
    "#### The tuning function\n",
    "The following function integrates all steps required for the hyperoptimization tuning. The results are stored in the model and the tuning score is printed per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144da29f-81b6-4bb8-861c-b990c9c87a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the classification\n",
    "def hp_xgbclass(space, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):\n",
    "    \"\"\" Function for Bayesian hyperparameter optimisation of XGboost classification model.\n",
    "    Accepts a parameter `space` and the training and testing data set.\n",
    "        :param tuning_space: a catalog containing the parameters used for hyperparameter tuning, ranges given in hp objects\n",
    "        :param X_train: accepts the train data\n",
    "        :param X_test: accepts the test data\n",
    "        :param X_eval: accepts the eval data used for optimisation\n",
    "        :param y_train: accepts the train target\n",
    "        :param y_test: accepts the test target\n",
    "        :param y_eval: accepts the eval data used for optimisation\n",
    "    \"\"\"\n",
    "\n",
    "    if (space['objective'] == 'binary:logistic') and (space['booster'] == 'gbtree'):\n",
    "        clf=xgb.XGBClassifier(\n",
    "            eta = space['eta'],\n",
    "            eval_metric = space['eval_metric'],\n",
    "            booster = space['booster'],\n",
    "            n_estimators = np.int64(space['n_estimators']),\n",
    "            max_depth = np.int64(space['max_depth']),\n",
    "            gamma = space['gamma'],\n",
    "            reg_alpha = space['reg_alpha'],\n",
    "            min_child_weight=np.int64(space['min_child_weight']),\n",
    "            colsample_bytree=space['colsample_bytree'],\n",
    "            colsample_bylevel=space['colsample_bylevel'],\n",
    "            colsample_bynode=space['colsample_bynode'],\n",
    "            early_stopping_rounds = space['early_stopping_rounds'],\n",
    "            objective = space['objective'],\n",
    "            # scale_pos_weight = space['scale_pos_weight'], # not used in this model\n",
    "            max_delta_step = space['max_delta_step'],\n",
    "            subsample = space['subsample']\n",
    "    \n",
    "        )\n",
    "          \n",
    "    clf.fit(X_train,\n",
    "            y_train,\n",
    "            eval_set = [(X_test, y_test)],\n",
    "            verbose=False)\n",
    "        \n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred > 0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fbd217-11b2-477c-a7b3-0cd138977ef5",
   "metadata": {},
   "source": [
    "#### The tuning\n",
    "In this section, the hyperparameter tuning is performed. The results are stored in the trial db and the hyperparameter file for later re-use. In case, the downstream analysis is changed, the model could be loaded in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c54e61-8338-450f-9d02-8e932ed27244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the crossvalidation or load stored parameters\n",
    "\n",
    "# lets check the run time\n",
    "start_time = time.monotonic()\n",
    "\n",
    "# Suppress warnings because of deprecated functions or parameters, we cannot influence\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Rerun (True) or get stored parameters (False)\n",
    "cv_rerun = True\n",
    "\n",
    "# define the stored files\n",
    "out_file_trial = './data/XGboost_model/231030_XGboost_gbtree_mlogloss_grav_non_encoded'\n",
    "out_file_params = './data/XGboost_model/231030_XGboost_gbtree_mlogloss_hyperparameters_grav_non_encoded'\n",
    "\n",
    "\n",
    "# rerun the crossvalidation\n",
    "if (cv_rerun == True):\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(fn = hp_xgbclass,\n",
    "                            space = tuning_space,\n",
    "                            algo = tpe.suggest,\n",
    "                            max_evals = 100,\n",
    "                            trials = trials,\n",
    "                            verbose=0\n",
    "                           )\n",
    "    # save the best parameters and the trials database\n",
    "    with open(out_file_trial, 'wb') as f:\n",
    "        pickle.dump(trials, f)\n",
    "\n",
    "    with open(out_file_params, 'wb') as f:\n",
    "        pickle.dump(best_hyperparams, f)\n",
    "\n",
    "# get the stored hyperparameters\n",
    "else:\n",
    "    with open('./data/XGboost_model/231030_XGboost_gbtree_mlogloss_hyperparameters_grav_raw', 'rb') as f:\n",
    "        best_hyperparams = pickle.load(f)\n",
    "\n",
    "# Track the end of the tuning\n",
    "end_time = time.monotonic()\n",
    "print(timedelta(seconds=end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8684917-11ae-4664-819c-2d87f36c34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05242bdd-d2ca-4447-bf23-ec396d4b99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_1 = xgb.XGBClassifier(\n",
    "    booster = tuning_space['booster'],\n",
    "    eta = best_hyperparams['eta'],\n",
    "    gamma = best_hyperparams['gamma'],\n",
    "    max_depth = np.int64(best_hyperparams['max_depth']),\n",
    "    min_child_weight = np.int64(best_hyperparams['min_child_weight']),\n",
    "    reg_alpha = best_hyperparams['reg_alpha'],\n",
    "    reg_lambda = best_hyperparams['reg_lambda'],\n",
    "    eval_metric = tuning_space['eval_metric'],\n",
    "    objective = tuning_space['objective'],\n",
    "    seed = tuning_space['seed'],\n",
    "    early_stopping_rounds = tuning_space['early_stopping_rounds'],\n",
    "    n_estimators = np.int64(best_hyperparams['n_estimators']),\n",
    "    subsample = best_hyperparams['subsample'],\n",
    "    max_delta_step = best_hyperparams['max_delta_step'],\n",
    "    colsample_bylevel = best_hyperparams['colsample_bylevel'],\n",
    "    colsample_bynode = best_hyperparams['colsample_bynode'],\n",
    "    colsample_bytree = best_hyperparams['colsample_bytree']\n",
    "        \n",
    "    )\n",
    "\n",
    "xgb_1 = xgb_1.fit(X_train,\n",
    "          y_train,\n",
    "          eval_set = [(X_test, y_test)], \n",
    "          verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78936a52-21c8-443a-80b6-a19289ecccc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance_types = ['total_gain', 'gain', 'weight', 'cover', 'total_cover']\n",
    "\n",
    "for f in importance_types:\n",
    "    xgb.plot_importance(xgb_1, max_num_features=50, importance_type=f, title=f)\n",
    "    plt.figure(figsize = (16, 12))\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9b563-5fcd-4744-adca-129cbf69342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_1.predict(X_test)\n",
    "y_pred = [round(value) for value in y_pred]\n",
    "\n",
    "print(classification_report(y_pred, y_test))\n",
    "\n",
    "print(pd.crosstab(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee44902-9a6d-495b-a3cf-74eb43dc3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize = (5, 4))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d5589-62fa-4c77-9a61-7143e1dc169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_tree\n",
    "import graphviz\n",
    "plot_tree(xgb_1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "516239a0-49f6-4d87-90ab-b461cbd4b57b",
   "metadata": {},
   "source": [
    "xgb.to_graphviz(xgb_1, num_trees=5, rankdir='TB' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aef1fc-ee62-4552-92ab-7710fe24b9f1",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] [Bergstra, J., Yamins, D., Cox, D. D. (2013) Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. TProc. of the 30th International Conference on Machine Learning (ICML 2013), June 2013, pp. I-115 to I-23.](https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)\n",
    "\n",
    "[2] https://towardsdatascience.com/hyperopt-demystified-3e14006eb6fa\n",
    "\n",
    "[3] https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html\n",
    "\n",
    "[4] https://github.com/hyperopt/hyperopt\n",
    "\n",
    "[5] https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning\n",
    "\n",
    "[6] https://github.com/hyperopt/hyperopt/wiki/FMin\n",
    "\n",
    "[7] https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
    "\n",
    "[8] https://www.datamachines.io/blog/auc-vs-log-loss\n",
    "\n",
    "[9] [Rashmi, K.V., Gilad-Bachrach (2015) DART: Dropouts meet Multiple Additive Regression Trees. arXiv:1505.01866v](https://doi.org/10.48550/arXiv.1505.01866)\n",
    "\n",
    "[10] https://datamachines.com/blog/auc-vs-log-loss\n",
    "\n",
    "[11] https://stats.stackexchange.com/questions/322408/logloss-vs-gini-auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
