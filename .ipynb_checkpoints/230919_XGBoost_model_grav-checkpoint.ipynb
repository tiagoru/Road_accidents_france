{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712d9d1c-1eeb-447d-bcfa-f5c3198b7713",
   "metadata": {},
   "source": [
    "# Modelling of France Accidents\n",
    "\n",
    "**Cohort:** mar23_accidents\n",
    "\n",
    "**Author:** Tobias Schulze\n",
    "\n",
    "**Date:** 14 September 2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6df835-aa14-4775-90df-de5512f881c4",
   "metadata": {},
   "source": [
    "## Loading of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22cfc8cd-2dd7-45e2-bd75-df68ef38c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import pickle\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d12d86a-94ac-4349-b68f-2b8a40e21ebe",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aaa9044-2e16-4821-a259-0e2b8fd2b259",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/230907_basic_table_for_analysis_cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, low_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, index_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32mparsers.pyx:825\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:913\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/230907_basic_table_for_analysis_cleaned.csv', low_memory = False, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552176c-9a85-4ead-ad23-1edfb5c1d448",
   "metadata": {},
   "source": [
    "## Data description\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e361f45-f63c-48a3-bc60-777a1d287f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fbf44b-f1ca-4d02-9b27-78827b41985d",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb68a0-55d1-461b-87e2-72833557b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258ad30-b0f4-4b86-9322-808ff955593e",
   "metadata": {},
   "source": [
    "### Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24315f42-0b75-439c-92a1-ed41d3b2c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e466b8d-e6d7-432a-bcd8-da9e0d60c3c5",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "### Transform data types\n",
    "#### Date and time variables\n",
    "The date and time variables are maybe important as grouping variables or as contrains for time dependent severity of accidents.\n",
    "\n",
    "For the grouping, a timestamp is required for unbiased identification time related accidents.\n",
    "\n",
    "_Problem_:\n",
    "\n",
    "During the transformation of the `hrmn` variable, I got aware, that the string contain integers like `1`,  `801`, or `1300`. Hence, anytime during data conversion, the colon got lost and the values got truncated. Hence `1` should be `00:01` and so on. Therefore, we need an additional transformation of the truncated data to `h:m` format.\n",
    "\n",
    "**Steps:**\n",
    "1. Fixing the truncated values in `hrmn`\n",
    "3. Creation of a `datatime` variable in format y-m-d hh:mm\n",
    "4. Transformation of the datatime varible to a `timestamp` variable\n",
    "\n",
    "\n",
    "In addition, we need to transform the type of `an_nais` to integer and `date` to `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e905ac-5f4e-4afc-b0e6-f1057093deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the hrmn issue:\n",
    "# Remove the colon\n",
    "df['hrmn'] = df.apply(lambda x: re.sub(string=x['hrmn'], pattern=':', repl=''), axis = 1)\n",
    "\n",
    "# Pad the string to four zeros\n",
    "df['hrmn'] = df.apply(lambda x: x['hrmn'].zfill(4), axis = 1)\n",
    "\n",
    "# Transform the variable to 'hh:mm' and split to hours and minutes\n",
    "df = df.assign(hrmn = pd.to_datetime(df['hrmn'], format='%H%M').dt.strftime('%H:%M'))\n",
    "\n",
    "# Create the daytime variable\n",
    "df['datetime'] = df.apply(lambda x: datetime(x['an'], x['mois'], x['jour'], datetime.strptime(x['hrmn'], \"%H:%M\").hour, datetime.strptime(x['hrmn'], \"%H:%M\").minute), axis = 1)\n",
    "\n",
    "# Create the timestamp\n",
    "df['timestamp'] = df.apply(lambda x: datetime.timestamp(x['datetime']), axis = 1)\n",
    "\n",
    "# Transform `an_nais`\n",
    "df['an_nais'] = df['an_nais'].astype('int64')\n",
    "\n",
    "# Transform `date`\n",
    "df = df.assign(date = pd.to_datetime(df['date'], format='mixed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50abfde-c053-45b5-a270-ae9adc390856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee2b4b-2bca-4686-b48f-b0f0d43b4bed",
   "metadata": {},
   "source": [
    "### Drop variables\n",
    "The dataset still contains variables with missing values. Some of them are important for possible mapping or a fine granulated classification.\n",
    "\n",
    "- `adr`: The address variable has missing values and is very fuzzy, so encoding might not be successfully.\n",
    "\n",
    "- `lat` and `lat`: not available for all sites\n",
    "\n",
    "- `holiday`: classifies the holiday, but will be used only, if `is_holiday` is relevant\n",
    "\n",
    "- `grav`: granulated classification of the target `fatal`\n",
    "\n",
    "- `datetime`: encoded in `timestamp`\n",
    "\n",
    "- `date`: encoded in separate datatime variables\n",
    "\n",
    "- `timestamp`: same as `date`\n",
    "\n",
    "- `etatp`: strongly correlate with `actp` and fine granulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055d7d0-b7ad-48e3-8b4e-3b41b58fede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bak = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967aff4c-b3a7-4ea6-8e48-80a148b62871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bak = df\n",
    "\n",
    "columns_drop = ['adr', 'lat', 'long', 'holiday', 'fatal', 'datetime', 'date', 'timestamp', 'etatp']\n",
    "\n",
    "df.drop(columns = columns_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4df31-4a36-442f-a1d7-ed33b603dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbcc7d6-db10-4449-a8c5-d6ed99b16c4d",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "For the first trial, use the `LabelEncoder` to encode categorial values. Then drop the old categorial values and replace them by the encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d039f3b-8cfd-4772-9f46-181d823edbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding with LabelEncoder\n",
    "encode_columns = ['actp', 'num_veh', 'hrmn', 'grav']\n",
    "encoded_df = df[encode_columns]\n",
    "encoded_df = encoded_df.astype('str')\n",
    "encoded_df = encoded_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Merge encoded values\n",
    "df_encoded = df\n",
    "df_encoded.drop(encode_columns, inplace=True, axis=1)\n",
    "df_encoded = pd.concat([encoded_df, df_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e9e70-1155-4884-ad0a-14a5ddb2e213",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "It was decided to use only accidents in metropolitan France and Corse.\n",
    "\n",
    "In preprocessing, the varible `metropolitan` with values `[0,1]` was created.\n",
    "\n",
    "Now, the data is fitered by this variable and then it is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4abde8-b613-4835-9057-dafef5c72d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.loc[df_encoded['metropolitan'] == 1]\n",
    "df_encoded.drop('metropolitan', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa7642-a212-4ae8-a640-b5cbf603694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32c059-294b-4bd0-8351-c1ea9ba40080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994eda3-d365-4948-b226-3ae5d445ecc7",
   "metadata": {},
   "source": [
    "#### Marking categorial data\n",
    "XGboost enables the use of specific categorial data. Hence, the change of the type to `categorial` is required[3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403bbd9b-dd17-46a8-a125-02f3896cfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['actp', 'num_veh', 'hrmn', 'place', 'catu', 'sexe', 'trajet',\n",
    "       'locp', 'lum', 'agg', 'int', 'atm', 'col', 'dep',\n",
    "       'catr', 'circ', 'nbv', 'vosp', 'prof', 'plan', 'surf', 'infra', 'situ',\n",
    "       'senc', 'catv', 'occutc', 'obs', 'obsm', 'choc', 'manv',\n",
    "       'secu_used', 'secu_avail', 'is_holiday', 'grav']\n",
    "cat_df = df_encoded[cat_columns]\n",
    "cat_df = cat_df.astype('category')\n",
    "\n",
    "# Merge new type columns\n",
    "df_encoded.drop(cat_columns, inplace=True, axis=1)\n",
    "df_encoded = pd.concat([df_encoded, cat_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34a0e0-a85b-4447-9b8b-66576863946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0dd480-0df6-4a5f-a179-f5741ea5381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['is_holiday'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded7a68-a52c-48e3-9276-33f9f55f132d",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "### Correlation matrix\n",
    "To get a first glimpse on the possible contrains in the data, a correlation matrix is plotted.\n",
    "\n",
    "The data variables are dropped to remove redundany with `timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540e6aa-5575-4184-a603-870a2c680265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop date columns for the correlation matrix\n",
    "columns_drop = ['an', 'mois', 'jour']\n",
    "df_select = df_encoded.drop(columns_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79448e-c276-43b9-a87e-9d2ea2157253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "df_numeric_scaled = pd.DataFrame(scaler.fit_transform(df_select), columns=df_select.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88a9ea-8230-4d0e-ae55-77a749dfbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "correlation_matrix = df_numeric_scaled.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Generate the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "# Display the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712a786-07c6-4f26-94b5-640f756e2433",
   "metadata": {},
   "source": [
    "The correlation matrix shows some intervariable correlations, but no real importance of variables to predict the target `fatal`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19180e-4918-4d1f-922a-14525d1a1c29",
   "metadata": {},
   "source": [
    "### Preparation of the training and test datasets\n",
    "#### Split data\n",
    "The dataset is split by year slides. The years 2005 to 2019 are selected for the train dataset and the years 2020 and 2021 for the test dataset.\n",
    "\n",
    "~In addition, the test dataset is split to a train and evaluation dataset with an evaluation set of 20% of the testing data.~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2bc919-2233-4813-840b-b7b0bd7b582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to train and test data\n",
    "X_train = df_encoded.loc[df_encoded['an'] < 2020].drop('grav', axis = 1)\n",
    "X_test = df_encoded.loc[df_encoded['an'] > 2019].drop('grav', axis = 1)\n",
    "\n",
    "y_train = df_encoded['grav'].loc[df_encoded['an'] < 2020]\n",
    "y_test = df_encoded['grav'].loc[df_encoded['an'] > 2019]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "#split the train dataset to train and eval data\n",
    "#X_test, X_eval, y_test, y_eval = train_test_split(X_train, y_train, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26625aa3-4aa4-407e-af35-fc4f79e5d944",
   "metadata": {},
   "source": [
    "#### Data scaling\n",
    "Decision trees are not sensitive to different scales and thus scaling is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0cee2-929f-4711-8370-63b1a875f475",
   "metadata": {},
   "source": [
    "#### Create xgboost matrices\n",
    "xgboost requires a specific matrix format. Depends on the training method."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6da13135-86b0-41fb-a762-15da43fdd821",
   "metadata": {},
   "source": [
    "# Create the xgb matrices\n",
    "train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "#eval = xgb.DMatrix(data=X_eval, label=y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935884d9-2884-4e18-a40e-2f6d6d4c6a93",
   "metadata": {},
   "source": [
    "### Modelling the data using a XGBoost Classification Model with Tree-based Parzen Estimators optimisation\n",
    "The Tree-based Parzen Estimator (TPE) [1] optimisation combines a Bayesian Sequential Model Based Optimisation (SMBO) and a random search on the hyperparemeter grid[2]. While the random search is a static approach, the SMBO optimises the model using prior runs to determine future points of exploration.\n",
    "\n",
    "The package `hyperopt` [4] is used for optimisation. The hyperparameter optimisation code is written based on ideas in [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899c3a5-71a0-4cc1-8b15-b8f860f89a90",
   "metadata": {},
   "source": [
    "#### The tuning space\n",
    "`hyperopt` uses a specific format of parameters expressions [7]. Hints on the parameter ranges are given in [1,5,7].\n",
    "\n",
    "The `booster` in this first model is `gbtree`. The evaluation metric is `logloss` or `auc`[8].\n",
    "\n",
    "In a later state, also Dropouts meet Multiple Additive Regression Trees (`dart`)[9] is tested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2c349-31d2-4b9e-9573-1e69ce36b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tuning space\n",
    "booster = 'gbtree'\n",
    "eval_metric = 'auc'\n",
    "\n",
    "tuning_space={\n",
    "    'eta': hp.uniform('eta', 0, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'gamma': hp.uniform ('gamma', 0, 10),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha', 0, 100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda', 0, 10),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0, 1),\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0, 1),\n",
    "    'colsample_bynode' : hp.uniform('colsample_bynode', 0, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 1000, 1),\n",
    "    'seed': 123,\n",
    "    'early_stopping_rounds': 10,\n",
    "    'objective': 'binary:logistic',\n",
    "    'subsample': hp.uniform('subsample', 0, 1)\n",
    "    }\n",
    "\n",
    "if (eval_metric == 'auc') or (eval_metric == 'logloss'):\n",
    "    tuning_space.update({'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 2)}) # helps in auc\n",
    "    tuning_space.update({'max_delta_step': hp.uniform('max_delta_step', 0, 2)}) # helps in auc\n",
    "    tuning_space.update({'eval_metric': eval_metric})\n",
    "    tuning_space.update({'booster': booster})\n",
    "\n",
    "if (booster == 'dart'):\n",
    "    tuning_space.update({'rate_drop': hp.uniform('rate_drop', 0, 1)})\n",
    "    tuning_space.update({'one_drop': 0})\n",
    "    tuning_space.update({'skip_drop': hp.uniform('skip_drop', 0, 1)})\n",
    "\n",
    "tuning_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b7c6d-e438-43cc-8ec7-2adb7f2e57aa",
   "metadata": {},
   "source": [
    "#### The tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144da29f-81b6-4bb8-861c-b990c9c87a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the classification\n",
    "def hp_xgbclass(space, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):\n",
    "    \"\"\" Function for Bayesian hyperparameter optimisation of XGboost classification model.\n",
    "    Accepts a parameter `space` and the training and testing data set.\n",
    "        :param tuning_space: a catalog containing the parameters used for hyperparameter tuning, ranges given in hp objects\n",
    "        :param X_train: accepts the train data\n",
    "        :param X_test: accepts the test data\n",
    "        :param X_eval: accepts the eval data used for optimisation\n",
    "        :param y_train: accepts the train target\n",
    "        :param y_test: accepts the test target\n",
    "        :param y_eval: accepts the eval data used for optimisation\n",
    "    \"\"\"\n",
    "\n",
    "    if (space['objective'] == 'binary:logistic') and (space['booster'] == 'gbtree'):\n",
    "        clf=xgb.XGBClassifier(\n",
    "            eta = space['eta'],\n",
    "            eval_metric = space['eval_metric'],\n",
    "            booster = space['booster'],\n",
    "            n_estimators = np.int64(space['n_estimators']),\n",
    "            max_depth = np.int64(space['max_depth']),\n",
    "            gamma = space['gamma'],\n",
    "            reg_alpha = space['reg_alpha'],\n",
    "            min_child_weight=np.int64(space['min_child_weight']),\n",
    "            colsample_bytree=space['colsample_bytree'],\n",
    "            colsample_bylevel=space['colsample_bylevel'],\n",
    "            colsample_bynode=space['colsample_bynode'],\n",
    "            early_stopping_rounds = space['early_stopping_rounds'],\n",
    "            objective = space['objective'],\n",
    "            scale_pos_weight = space['scale_pos_weight'],\n",
    "            max_delta_step = space['max_delta_step'],\n",
    "            subsample = space['subsample'],\n",
    "            enable_categorical = True,\n",
    "            verbosity = 0\n",
    "        )\n",
    "            \n",
    "\n",
    "    if (space['objective'] == 'binary:logistic') and (space['booster'] == 'dart'):\n",
    "        clf=xgb.XGBClassifier(\n",
    "            eta = space['eta'],\n",
    "            eval_metric = space['eval_metric'],\n",
    "            booster = space['booster'],\n",
    "            n_estimators = np.int64(space['n_estimators']),\n",
    "            max_depth = np.int64(space['max_depth']),\n",
    "            gamma = space['gamma'],\n",
    "            reg_alpha = space['reg_alpha'],\n",
    "            min_child_weight=np.int64(space['min_child_weight']),\n",
    "            colsample_bytree=space['colsample_bytree'],\n",
    "            colsample_bylevel=space['colsample_bylevel'],\n",
    "            colsample_bynode=space['colsample_bynode'],\n",
    "            early_stopping_rounds = space['early_stopping_rounds'],\n",
    "            objective = space['objective'],\n",
    "            scale_pos_weight = space['scale_pos_weight'],\n",
    "            max_delta_step = space['max_delta_step'],\n",
    "            subsample = space['subsample'],\n",
    "            rate_drop = space['rate_drop'],\n",
    "            one_drop = space['one_drop'],\n",
    "            skip_drop = space['skip_drop'],\n",
    "            enable_categorical = True,\n",
    "            verbosity = 0\n",
    "        )\n",
    "    \n",
    "\n",
    "    clf.fit(X_train,\n",
    "            y_train,\n",
    "            eval_set = [(X_test, y_test)],\n",
    "            verbose=False)\n",
    "        \n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred > 0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c54e61-8338-450f-9d02-8e932ed27244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the crossvalidation or load stored parameters\n",
    "\n",
    "# Rerun or get stored parameters\n",
    "cv_rerun = True\n",
    "\n",
    "# define the stored files\n",
    "# gbtree\n",
    "if (tuning_space['eval_metric'] == 'auc') and (tuning_space['booster'] == 'gbtree'):\n",
    "    out_file_trial = './data/XGboost_model/160923_XGboost_gbtree_auc_trial_db'\n",
    "    out_file_params = './data/XGboost_model/160923_XGboost_gbtree_auc_hyperparameters'\n",
    "    \n",
    "elif (tuning_space['eval_metric'] == 'lossless') and (tuning_space['booster'] == 'gbtree'):\n",
    "    out_file_trial = './data/XGboost_model/160923_XGboost_gbtree_lossless_trial_db'\n",
    "    out_file_params = './data/XGboost_model/160923_XGboost_gbtree_lossles_hyperparameters'\n",
    "\n",
    "# dart\n",
    "elif (tuning_space['eval_metric'] == 'auc') and (tuning_space['booster'] == 'dart'):\n",
    "    out_file_trial = './data/XGboost_model/160923_XGboost_dart_auc_trial_db'\n",
    "    out_file_params = './data/XGboost_model/160923_XGboost_dart_auc_hyperparameters'\n",
    "\n",
    "elif (tuning_space['eval_metric'] == 'lossless') and (tuning_space['booster'] == 'dart'):\n",
    "    out_file_trial = './data/XGboost_model/160923_XGboost_dart_lossless_trial_db'\n",
    "    out_file_params = './data/XGboost_model/160923_XGboost_dart_lossles_hyperparameters'\n",
    "\n",
    "\n",
    "# rerun the crossvalidation\n",
    "if (cv_rerun == True):\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(fn = hp_xgbclass,\n",
    "                            space = tuning_space,\n",
    "                            algo = tpe.suggest,\n",
    "                            max_evals = 100,\n",
    "                            trials = trials\n",
    "                           )\n",
    "    # save the best parameters and the trials database\n",
    "    with open(out_file_trial, 'wb') as f:\n",
    "        pickle.dump(trials, f)\n",
    "\n",
    "    with open(out_file_params, 'wb') as f:\n",
    "        pickle.dump(best_hyperparams, f)\n",
    "\n",
    "# get the stored hyperparameters\n",
    "else:\n",
    "    with open('./data/XGboost_model/160923_XGboost_gbtree_auc_hyperparameters', 'rb') as f:\n",
    "        best_hyperparams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8684917-11ae-4664-819c-2d87f36c34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05242bdd-d2ca-4447-bf23-ec396d4b99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_space['booster'] == \"gbtree\"):\n",
    "     xgb_1 = xgb.XGBClassifier(\n",
    "        booster = tuning_space['booster'],\n",
    "        eta = best_hyperparams['eta'],\n",
    "        gamma = best_hyperparams['gamma'],\n",
    "        max_depth = np.int64(best_hyperparams['max_depth']),\n",
    "        min_child_weight = np.int64(best_hyperparams['min_child_weight']),\n",
    "        reg_alpha = best_hyperparams['reg_alpha'],\n",
    "        reg_lambda = best_hyperparams['reg_lambda'],\n",
    "        eval_metric = tuning_space['eval_metric'],\n",
    "        objective = tuning_space['objective'],\n",
    "        seed = tuning_space['seed'],\n",
    "        early_stopping_rounds = tuning_space['early_stopping_rounds'],\n",
    "        n_estimators = np.int64(best_hyperparams['n_estimators']),\n",
    "        subsample = best_hyperparams['subsample'],\n",
    "        scale_pos_weight = best_hyperparams['scale_pos_weight'],\n",
    "        max_delta_step = best_hyperparams['max_delta_step'],\n",
    "        colsample_bylevel = best_hyperparams['colsample_bylevel'],\n",
    "        colsample_bynode = best_hyperparams['colsample_bynode'],\n",
    "        colsample_bytree = best_hyperparams['colsample_bytree'],\n",
    "        enable_categorical = True\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "if (tuning_space['booster'] == \"dart\"):\n",
    "     xgb_1 = xgb.XGBClassifier(\n",
    "        booster = tuning_space['booster'],\n",
    "        eta = best_hyperparams['eta'],\n",
    "        gamma = best_hyperparams['gamma'],\n",
    "        max_depth = np.int64(best_hyperparams['max_depth']),\n",
    "        min_child_weight = np.int64(best_hyperparams['min_child_weight']),\n",
    "        reg_alpha = best_hyperparams['reg_alpha'],\n",
    "        reg_lambda = best_hyperparams['reg_lambda'],\n",
    "        eval_metric = tuning_space['eval_metric'],\n",
    "        objective = tuning_space['objective'],\n",
    "        seed = tuning_space['seed'],\n",
    "        early_stopping_rounds = tuning_space['early_stopping_rounds'],\n",
    "        n_estimators = np.int64(best_hyperparams['n_estimators']),\n",
    "        subsample = best_hyperparams['subsample'],\n",
    "        scale_pos_weight = best_hyperparams['scale_pos_weight'],\n",
    "        max_delta_step = best_hyperparams['max_delta_step'],\n",
    "        colsample_bylevel = best_hyperparams['colsample_bylevel'],\n",
    "        colsample_bynode = best_hyperparams['colsample_bynode'],\n",
    "        colsample_bytree = best_hyperparams['colsample_bytree'],\n",
    "        rate_drop = best_hyperparams['rate_drop'],\n",
    "        one_drop = space['one_drop'],\n",
    "        skip_drop = best_hyperparams['skip_drop'],\n",
    "        enable_categorical = True\n",
    "    )\n",
    "  \n",
    "xgb_1 = xgb_1.fit(X_train,\n",
    "          y_train,\n",
    "          eval_set = [(X_test, y_test)], \n",
    "          verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78936a52-21c8-443a-80b6-a19289ecccc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance_types = ['total_gain', 'gain', 'weight', 'cover', 'total_cover']\n",
    "\n",
    "for f in importance_types:\n",
    "    xgb.plot_importance(xgb_1, max_num_features=50, importance_type=f, title=f)\n",
    "    plt.figure(figsize = (16, 12))\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9b563-5fcd-4744-adca-129cbf69342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_1.predict(X_test)\n",
    "y_pred = [round(value) for value in y_pred]\n",
    "\n",
    "print(classification_report(y_pred, y_test))\n",
    "\n",
    "print(pd.crosstab(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee44902-9a6d-495b-a3cf-74eb43dc3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize = (5, 4))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ed08f-f878-4116-9552-b48ba57473c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d08253-e292-42c1-ab52-c8aeb085fd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d0548-7d90-48af-9e91-b00229b2826c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03aef1fc-ee62-4552-92ab-7710fe24b9f1",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] [Bergstra, J., Yamins, D., Cox, D. D. (2013) Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. TProc. of the 30th International Conference on Machine Learning (ICML 2013), June 2013, pp. I-115 to I-23.](https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)\n",
    "\n",
    "[2] https://towardsdatascience.com/hyperopt-demystified-3e14006eb6fa\n",
    "\n",
    "[3] https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html\n",
    "\n",
    "[4] https://github.com/hyperopt/hyperopt\n",
    "\n",
    "[5] https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning\n",
    "\n",
    "[6] https://github.com/hyperopt/hyperopt/wiki/FMin\n",
    "\n",
    "[7] https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
    "\n",
    "[8] https://www.datamachines.io/blog/auc-vs-log-loss\n",
    "\n",
    "[9] [Rashmi, K.V., Gilad-Bachrach (2015) DART: Dropouts meet Multiple Additive Regression Trees. arXiv:1505.01866v](https://doi.org/10.48550/arXiv.1505.01866)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
