{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712d9d1c-1eeb-447d-bcfa-f5c3198b7713",
   "metadata": {},
   "source": [
    "# Modelling of France Accidents\n",
    "\n",
    "**Cohort:** mar23_accidents\n",
    "\n",
    "**Author:** Tobias Schulze\n",
    "\n",
    "**Date:** 20 September 2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7195ca-2ecc-4843-a50e-809db0cbd06a",
   "metadata": {},
   "source": [
    "## Modelling approach\n",
    "\n",
    "- `fatal` is target\n",
    "- select accidents with participation of bicycles only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6df835-aa14-4775-90df-de5512f881c4",
   "metadata": {},
   "source": [
    "## Loading of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22cfc8cd-2dd7-45e2-bd75-df68ef38c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import pickle\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d12d86a-94ac-4349-b68f-2b8a40e21ebe",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aaa9044-2e16-4821-a259-0e2b8fd2b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/230907_basic_table_for_analysis_cleaned.csv', low_memory = False, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552176c-9a85-4ead-ad23-1edfb5c1d448",
   "metadata": {},
   "source": [
    "## Data description\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e361f45-f63c-48a3-bc60-777a1d287f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_veh</th>\n",
       "      <th>place</th>\n",
       "      <th>catu</th>\n",
       "      <th>grav</th>\n",
       "      <th>sexe</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>trajet</th>\n",
       "      <th>locp</th>\n",
       "      <th>actp</th>\n",
       "      <th>etatp</th>\n",
       "      <th>...</th>\n",
       "      <th>obs</th>\n",
       "      <th>obsm</th>\n",
       "      <th>choc</th>\n",
       "      <th>manv</th>\n",
       "      <th>date</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>secu_used</th>\n",
       "      <th>secu_avail</th>\n",
       "      <th>fatal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_Acc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201900000001</th>\n",
       "      <td>B01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201900000001</th>\n",
       "      <td>B01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201900000001</th>\n",
       "      <td>A01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201900000002</th>\n",
       "      <td>A01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201900000003</th>\n",
       "      <td>A01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             num_veh  place  catu  grav  sexe  an_nais  trajet  locp actp  \\\n",
       "Num_Acc                                                                     \n",
       "201900000001     B01    2.0     2     4     2   2002.0     0.0  -1.0   -1   \n",
       "201900000001     B01    1.0     1     4     2   1993.0     5.0  -1.0   -1   \n",
       "201900000001     A01    1.0     1     1     1   1959.0     0.0  -1.0   -1   \n",
       "201900000002     A01    1.0     1     4     2   1994.0     0.0  -1.0   -1   \n",
       "201900000003     A01    1.0     1     1     1   1996.0     0.0  -1.0    0   \n",
       "\n",
       "              etatp  ...  obs  obsm  choc  manv        date  is_holiday  \\\n",
       "Num_Acc              ...                                                  \n",
       "201900000001   -1.0  ...  0.0   2.0   5.0  23.0  2019-11-30       False   \n",
       "201900000001   -1.0  ...  0.0   2.0   5.0  23.0  2019-11-30       False   \n",
       "201900000001   -1.0  ...  1.0   0.0   3.0  11.0  2019-11-30       False   \n",
       "201900000002   -1.0  ...  4.0   0.0   1.0   0.0  2019-11-30       False   \n",
       "201900000003   -1.0  ...  0.0   2.0   1.0   2.0  2019-11-28       False   \n",
       "\n",
       "              holiday  secu_used  secu_avail fatal  \n",
       "Num_Acc                                             \n",
       "201900000001      NaN          3           1     0  \n",
       "201900000001      NaN          3           1     0  \n",
       "201900000001      NaN          3           1     0  \n",
       "201900000002      NaN          3           1     0  \n",
       "201900000003      NaN          3           1     0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fbf44b-f1ca-4d02-9b27-78827b41985d",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bb68a0-55d1-461b-87e2-72833557b4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2509596 entries, 201900000001 to 201800057783\n",
      "Data columns (total 46 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   num_veh       object \n",
      " 1   place         float64\n",
      " 2   catu          int64  \n",
      " 3   grav          int64  \n",
      " 4   sexe          int64  \n",
      " 5   an_nais       float64\n",
      " 6   trajet        float64\n",
      " 7   locp          float64\n",
      " 8   actp          object \n",
      " 9   etatp         float64\n",
      " 10  an            int64  \n",
      " 11  mois          int64  \n",
      " 12  jour          int64  \n",
      " 13  hrmn          object \n",
      " 14  lum           int64  \n",
      " 15  agg           int64  \n",
      " 16  int           int64  \n",
      " 17  atm           float64\n",
      " 18  col           float64\n",
      " 19  adr           object \n",
      " 20  lat           float64\n",
      " 21  long          float64\n",
      " 22  dep           int64  \n",
      " 23  metropolitan  float64\n",
      " 24  catr          float64\n",
      " 25  circ          float64\n",
      " 26  nbv           float64\n",
      " 27  vosp          float64\n",
      " 28  prof          float64\n",
      " 29  plan          float64\n",
      " 30  surf          float64\n",
      " 31  infra         float64\n",
      " 32  situ          float64\n",
      " 33  senc          float64\n",
      " 34  catv          int64  \n",
      " 35  occutc        float64\n",
      " 36  obs           float64\n",
      " 37  obsm          float64\n",
      " 38  choc          float64\n",
      " 39  manv          float64\n",
      " 40  date          object \n",
      " 41  is_holiday    bool   \n",
      " 42  holiday       object \n",
      " 43  secu_used     int64  \n",
      " 44  secu_avail    int64  \n",
      " 45  fatal         int64  \n",
      "dtypes: bool(1), float64(25), int64(14), object(6)\n",
      "memory usage: 883.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258ad30-b0f4-4b86-9322-808ff955593e",
   "metadata": {},
   "source": [
    "### Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24315f42-0b75-439c-92a1-ed41d3b2c33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_veh               0\n",
       "place                 0\n",
       "catu                  0\n",
       "grav                  0\n",
       "sexe                  0\n",
       "an_nais               0\n",
       "trajet                0\n",
       "locp                  0\n",
       "actp                  0\n",
       "etatp                 0\n",
       "an                    0\n",
       "mois                  0\n",
       "jour                  0\n",
       "hrmn                  0\n",
       "lum                   0\n",
       "agg                   0\n",
       "int                   0\n",
       "atm                   0\n",
       "col                   0\n",
       "adr              343702\n",
       "lat             1065871\n",
       "long            1073114\n",
       "dep                   0\n",
       "metropolitan          0\n",
       "catr                  0\n",
       "circ                  0\n",
       "nbv                   0\n",
       "vosp                  0\n",
       "prof                  0\n",
       "plan                  0\n",
       "surf                  0\n",
       "infra                 0\n",
       "situ                  0\n",
       "senc                  0\n",
       "catv                  0\n",
       "occutc                0\n",
       "obs                   0\n",
       "obsm                  0\n",
       "choc                  0\n",
       "manv                  0\n",
       "date                  0\n",
       "is_holiday            0\n",
       "holiday         2453128\n",
       "secu_used             0\n",
       "secu_avail            0\n",
       "fatal                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e466b8d-e6d7-432a-bcd8-da9e0d60c3c5",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "### Transform data types\n",
    "#### Date and time variables\n",
    "The date and time variables are maybe important as grouping variables or as contrains for time dependent severity of accidents.\n",
    "\n",
    "For the grouping, a timestamp is required for unbiased identification time related accidents.\n",
    "\n",
    "_Problem_:\n",
    "\n",
    "During the transformation of the `hrmn` variable, I got aware, that the string contain integers like `1`,  `801`, or `1300`. Hence, anytime during data conversion, the colon got lost and the values got truncated. Hence `1` should be `00:01` and so on. Therefore, we need an additional transformation of the truncated data to `h:m` format.\n",
    "\n",
    "**Steps:**\n",
    "1. Fixing the truncated values in `hrmn`\n",
    "3. Creation of a `datatime` variable in format y-m-d hh:mm\n",
    "4. Transformation of the datatime varible to a `timestamp` variable\n",
    "\n",
    "\n",
    "In addition, we need to transform the type of `an_nais` to integer and `date` to `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e905ac-5f4e-4afc-b0e6-f1057093deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the hrmn issue:\n",
    "# Remove the colon\n",
    "df['hrmn'] = df.apply(lambda x: re.sub(string=x['hrmn'], pattern=':', repl=''), axis = 1)\n",
    "\n",
    "# Pad the string to four zeros\n",
    "df['hrmn'] = df.apply(lambda x: x['hrmn'].zfill(4), axis = 1)\n",
    "\n",
    "# Transform the variable to 'hh:mm' and split to hours and minutes\n",
    "df = df.assign(hrmn = pd.to_datetime(df['hrmn'], format='%H%M').dt.strftime('%H:%M'))\n",
    "\n",
    "# Create the daytime variable\n",
    "df['datetime'] = df.apply(lambda x: datetime(x['an'], x['mois'], x['jour'], datetime.strptime(x['hrmn'], \"%H:%M\").hour, datetime.strptime(x['hrmn'], \"%H:%M\").minute), axis = 1)\n",
    "\n",
    "# Create the timestamp\n",
    "df['timestamp'] = df.apply(lambda x: datetime.timestamp(x['datetime']), axis = 1)\n",
    "\n",
    "# Transform `an_nais`\n",
    "df['an_nais'] = df['an_nais'].astype('int64')\n",
    "\n",
    "# Transform `date`\n",
    "df = df.assign(date = pd.to_datetime(df['date'], format='mixed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50abfde-c053-45b5-a270-ae9adc390856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2509596 entries, 201900000001 to 201800057783\n",
      "Data columns (total 48 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   num_veh       object        \n",
      " 1   place         float64       \n",
      " 2   catu          int64         \n",
      " 3   grav          int64         \n",
      " 4   sexe          int64         \n",
      " 5   an_nais       int64         \n",
      " 6   trajet        float64       \n",
      " 7   locp          float64       \n",
      " 8   actp          object        \n",
      " 9   etatp         float64       \n",
      " 10  an            int64         \n",
      " 11  mois          int64         \n",
      " 12  jour          int64         \n",
      " 13  hrmn          object        \n",
      " 14  lum           int64         \n",
      " 15  agg           int64         \n",
      " 16  int           int64         \n",
      " 17  atm           float64       \n",
      " 18  col           float64       \n",
      " 19  adr           object        \n",
      " 20  lat           float64       \n",
      " 21  long          float64       \n",
      " 22  dep           int64         \n",
      " 23  metropolitan  float64       \n",
      " 24  catr          float64       \n",
      " 25  circ          float64       \n",
      " 26  nbv           float64       \n",
      " 27  vosp          float64       \n",
      " 28  prof          float64       \n",
      " 29  plan          float64       \n",
      " 30  surf          float64       \n",
      " 31  infra         float64       \n",
      " 32  situ          float64       \n",
      " 33  senc          float64       \n",
      " 34  catv          int64         \n",
      " 35  occutc        float64       \n",
      " 36  obs           float64       \n",
      " 37  obsm          float64       \n",
      " 38  choc          float64       \n",
      " 39  manv          float64       \n",
      " 40  date          datetime64[ns]\n",
      " 41  is_holiday    bool          \n",
      " 42  holiday       object        \n",
      " 43  secu_used     int64         \n",
      " 44  secu_avail    int64         \n",
      " 45  fatal         int64         \n",
      " 46  datetime      datetime64[ns]\n",
      " 47  timestamp     float64       \n",
      "dtypes: bool(1), datetime64[ns](2), float64(25), int64(15), object(5)\n",
      "memory usage: 921.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee2b4b-2bca-4686-b48f-b0f0d43b4bed",
   "metadata": {},
   "source": [
    "### Drop variables\n",
    "The dataset still contains variables with missing values. Some of them are important for possible mapping or a fine granulated classification.\n",
    "\n",
    "- `adr`: The address variable has missing values and is very fuzzy, so encoding might not be successfully.\n",
    "\n",
    "- `lat` and `lat`: not available for all sites\n",
    "\n",
    "- `holiday`: classifies the holiday, but will be used only, if `is_holiday` is relevant\n",
    "\n",
    "- `grav`: granulated classification of the target `fatal`\n",
    "\n",
    "- `datetime`: encoded in `timestamp`\n",
    "\n",
    "- `date`: encoded in separate datatime variables\n",
    "\n",
    "- `timestamp`: same as `date`\n",
    "\n",
    "- `etatp`: strongly correlate with `actp` and fine granulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3055d7d0-b7ad-48e3-8b4e-3b41b58fede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bak = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967aff4c-b3a7-4ea6-8e48-80a148b62871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bak = df\n",
    "\n",
    "columns_drop = ['adr', 'lat', 'long', 'holiday', 'datetime', 'date', 'timestamp', 'etatp']\n",
    "\n",
    "df.drop(columns = columns_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab4df31-4a36-442f-a1d7-ed33b603dc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2509596 entries, 201900000001 to 201800057783\n",
      "Data columns (total 40 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   num_veh       object \n",
      " 1   place         float64\n",
      " 2   catu          int64  \n",
      " 3   grav          int64  \n",
      " 4   sexe          int64  \n",
      " 5   an_nais       int64  \n",
      " 6   trajet        float64\n",
      " 7   locp          float64\n",
      " 8   actp          object \n",
      " 9   an            int64  \n",
      " 10  mois          int64  \n",
      " 11  jour          int64  \n",
      " 12  hrmn          object \n",
      " 13  lum           int64  \n",
      " 14  agg           int64  \n",
      " 15  int           int64  \n",
      " 16  atm           float64\n",
      " 17  col           float64\n",
      " 18  dep           int64  \n",
      " 19  metropolitan  float64\n",
      " 20  catr          float64\n",
      " 21  circ          float64\n",
      " 22  nbv           float64\n",
      " 23  vosp          float64\n",
      " 24  prof          float64\n",
      " 25  plan          float64\n",
      " 26  surf          float64\n",
      " 27  infra         float64\n",
      " 28  situ          float64\n",
      " 29  senc          float64\n",
      " 30  catv          int64  \n",
      " 31  occutc        float64\n",
      " 32  obs           float64\n",
      " 33  obsm          float64\n",
      " 34  choc          float64\n",
      " 35  manv          float64\n",
      " 36  is_holiday    bool   \n",
      " 37  secu_used     int64  \n",
      " 38  secu_avail    int64  \n",
      " 39  fatal         int64  \n",
      "dtypes: bool(1), float64(21), int64(15), object(3)\n",
      "memory usage: 768.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbcc7d6-db10-4449-a8c5-d6ed99b16c4d",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "For the first trial, use the `LabelEncoder` to encode categorial values. Then drop the old categorial values and replace them by the encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d039f3b-8cfd-4772-9f46-181d823edbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding with LabelEncoder\n",
    "encode_columns = ['actp', 'num_veh', 'hrmn']\n",
    "encoded_df = df[encode_columns]\n",
    "encoded_df = encoded_df.astype('str')\n",
    "encoded_df = encoded_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Merge encoded values\n",
    "df_encoded = df\n",
    "df_encoded.drop(encode_columns, inplace=True, axis=1)\n",
    "df_encoded = pd.concat([encoded_df, df_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e9e70-1155-4884-ad0a-14a5ddb2e213",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "It was decided to use only accidents in metropolitan France and Corse.\n",
    "\n",
    "In preprocessing, the varible `metropolitan` with values `[0,1]` was created.\n",
    "\n",
    "Now, the data is fitered by this variable and then it is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4abde8-b613-4835-9057-dafef5c72d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.loc[df_encoded['metropolitan'] == 1]\n",
    "df_encoded.drop('metropolitan', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdaa7642-a212-4ae8-a640-b5cbf603694c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actp          0\n",
       "num_veh       0\n",
       "hrmn          0\n",
       "place         0\n",
       "catu          0\n",
       "grav          0\n",
       "sexe          0\n",
       "an_nais       0\n",
       "trajet        0\n",
       "locp          0\n",
       "an            0\n",
       "mois          0\n",
       "jour          0\n",
       "lum           0\n",
       "agg           0\n",
       "int           0\n",
       "atm           0\n",
       "col           0\n",
       "dep           0\n",
       "catr          0\n",
       "circ          0\n",
       "nbv           0\n",
       "vosp          0\n",
       "prof          0\n",
       "plan          0\n",
       "surf          0\n",
       "infra         0\n",
       "situ          0\n",
       "senc          0\n",
       "catv          0\n",
       "occutc        0\n",
       "obs           0\n",
       "obsm          0\n",
       "choc          0\n",
       "manv          0\n",
       "is_holiday    0\n",
       "secu_used     0\n",
       "secu_avail    0\n",
       "fatal         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da32c059-294b-4bd0-8351-c1ea9ba40080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2421738 entries, 201900000001 to 201800055766\n",
      "Data columns (total 39 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   actp        int64  \n",
      " 1   num_veh     int64  \n",
      " 2   hrmn        int64  \n",
      " 3   place       float64\n",
      " 4   catu        int64  \n",
      " 5   grav        int64  \n",
      " 6   sexe        int64  \n",
      " 7   an_nais     int64  \n",
      " 8   trajet      float64\n",
      " 9   locp        float64\n",
      " 10  an          int64  \n",
      " 11  mois        int64  \n",
      " 12  jour        int64  \n",
      " 13  lum         int64  \n",
      " 14  agg         int64  \n",
      " 15  int         int64  \n",
      " 16  atm         float64\n",
      " 17  col         float64\n",
      " 18  dep         int64  \n",
      " 19  catr        float64\n",
      " 20  circ        float64\n",
      " 21  nbv         float64\n",
      " 22  vosp        float64\n",
      " 23  prof        float64\n",
      " 24  plan        float64\n",
      " 25  surf        float64\n",
      " 26  infra       float64\n",
      " 27  situ        float64\n",
      " 28  senc        float64\n",
      " 29  catv        int64  \n",
      " 30  occutc      float64\n",
      " 31  obs         float64\n",
      " 32  obsm        float64\n",
      " 33  choc        float64\n",
      " 34  manv        float64\n",
      " 35  is_holiday  bool   \n",
      " 36  secu_used   int64  \n",
      " 37  secu_avail  int64  \n",
      " 38  fatal       int64  \n",
      "dtypes: bool(1), float64(20), int64(18)\n",
      "memory usage: 722.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994eda3-d365-4948-b226-3ae5d445ecc7",
   "metadata": {},
   "source": [
    "#### Marking categorial data\n",
    "XGboost enables the use of specific categorial data. Hence, the change of the type to `categorial` is required[3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "403bbd9b-dd17-46a8-a125-02f3896cfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['actp', 'num_veh', 'hrmn', 'place', 'catu', 'sexe', 'trajet',\n",
    "       'locp', 'lum', 'agg', 'int', 'atm', 'col', 'dep',\n",
    "       'catr', 'circ', 'nbv', 'vosp', 'prof', 'plan', 'surf', 'infra', 'situ',\n",
    "       'senc', 'catv', 'occutc', 'obs', 'obsm', 'choc', 'manv',\n",
    "       'secu_used', 'secu_avail', 'fatal', 'is_holiday']\n",
    "cat_df = df_encoded[cat_columns]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cat_df = cat_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# cat_df = cat_df.astype('category')\n",
    "\n",
    "# Merge new type columns\n",
    "df_encoded.drop(cat_columns, inplace=True, axis=1)\n",
    "df_encoded = pd.concat([df_encoded, cat_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a34a0e0-a85b-4447-9b8b-66576863946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2421738 entries, 201900000001 to 201800055766\n",
      "Data columns (total 39 columns):\n",
      " #   Column      Dtype\n",
      "---  ------      -----\n",
      " 0   grav        int64\n",
      " 1   an_nais     int64\n",
      " 2   an          int64\n",
      " 3   mois        int64\n",
      " 4   jour        int64\n",
      " 5   actp        int64\n",
      " 6   num_veh     int64\n",
      " 7   hrmn        int64\n",
      " 8   place       int64\n",
      " 9   catu        int64\n",
      " 10  sexe        int64\n",
      " 11  trajet      int64\n",
      " 12  locp        int64\n",
      " 13  lum         int64\n",
      " 14  agg         int64\n",
      " 15  int         int64\n",
      " 16  atm         int64\n",
      " 17  col         int64\n",
      " 18  dep         int64\n",
      " 19  catr        int64\n",
      " 20  circ        int64\n",
      " 21  nbv         int64\n",
      " 22  vosp        int64\n",
      " 23  prof        int64\n",
      " 24  plan        int64\n",
      " 25  surf        int64\n",
      " 26  infra       int64\n",
      " 27  situ        int64\n",
      " 28  senc        int64\n",
      " 29  catv        int64\n",
      " 30  occutc      int64\n",
      " 31  obs         int64\n",
      " 32  obsm        int64\n",
      " 33  choc        int64\n",
      " 34  manv        int64\n",
      " 35  secu_used   int64\n",
      " 36  secu_avail  int64\n",
      " 37  fatal       int64\n",
      " 38  is_holiday  int64\n",
      "dtypes: int64(39)\n",
      "memory usage: 739.1 MB\n"
     ]
    }
   ],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ba7d7-5155-4835-b40f-3c472fd92b22",
   "metadata": {},
   "source": [
    "### Select accidents with participation of bicycles\n",
    "\n",
    "- create a index column and groupby\n",
    "- filter the groups containing a value of `2` in `catv`\n",
    "- drop index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41571feb-6f63-4d08-b8f2-929976d65df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_groups(group):\n",
    "    return 2 in group['catv'].values\n",
    "\n",
    "df_encoded['index'] = df_encoded.index\n",
    "\n",
    "df_encoded = df_encoded.groupby('index').filter(filter_groups)\n",
    "df_encoded.drop('index', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54405f27-8f64-489b-be41-3627ee17098f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grav</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>actp</th>\n",
       "      <th>num_veh</th>\n",
       "      <th>hrmn</th>\n",
       "      <th>place</th>\n",
       "      <th>catu</th>\n",
       "      <th>...</th>\n",
       "      <th>catv</th>\n",
       "      <th>occutc</th>\n",
       "      <th>obs</th>\n",
       "      <th>obsm</th>\n",
       "      <th>choc</th>\n",
       "      <th>manv</th>\n",
       "      <th>secu_used</th>\n",
       "      <th>secu_avail</th>\n",
       "      <th>fatal</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_Acc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201900000037</th>\n",
       "      <td>4</td>\n",
       "      <td>1958</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>820</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201900000037</th>\n",
       "      <td>1</td>\n",
       "      <td>1969</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>820</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201900000038</th>\n",
       "      <td>4</td>\n",
       "      <td>1966</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201900000038</th>\n",
       "      <td>1</td>\n",
       "      <td>1986</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201900000049</th>\n",
       "      <td>4</td>\n",
       "      <td>1973</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              grav  an_nais    an  mois  jour  actp  num_veh  hrmn  place  \\\n",
       "Num_Acc                                                                     \n",
       "201900000037     4     1958  2019    11    29     1       14   820      2   \n",
       "201900000037     1     1969  2019    11    29     1        0   820      2   \n",
       "201900000038     4     1966  2019    11    29     0       14   830      2   \n",
       "201900000038     1     1986  2019    11    29     0        0   830      2   \n",
       "201900000049     4     1973  2019    11    28     0        0   496      2   \n",
       "\n",
       "              catu  ...  catv  occutc  obs  obsm  choc  manv  secu_used  \\\n",
       "Num_Acc             ...                                                   \n",
       "201900000037     0  ...     2       0    1     3     9    20          2   \n",
       "201900000037     0  ...    33       0    1     3     1     2          2   \n",
       "201900000038     0  ...     2       0    1     3     2     2          2   \n",
       "201900000038     0  ...     8       0    1     3     2    16          2   \n",
       "201900000049     0  ...     2       0   15     1     1     1          2   \n",
       "\n",
       "              secu_avail  fatal  is_holiday  \n",
       "Num_Acc                                      \n",
       "201900000037           0      0           0  \n",
       "201900000037           0      0           0  \n",
       "201900000038           0      0           0  \n",
       "201900000038           0      0           0  \n",
       "201900000049           0      0           0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded7a68-a52c-48e3-9276-33f9f55f132d",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "### Correlation matrix\n",
    "To get a first glimpse on the possible contrains in the data, a correlation matrix is plotted.\n",
    "\n",
    "The data variables are dropped to remove redundany with `timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1540e6aa-5575-4184-a603-870a2c680265",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['index'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Drop date columns for the correlation matrix\u001b[39;00m\n\u001b[1;32m      2\u001b[0m columns_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124man\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmois\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m df_select \u001b[38;5;241m=\u001b[39m df_encoded\u001b[38;5;241m.\u001b[39mdrop(columns_drop, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds1/lib/python3.11/site-packages/pandas/core/frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5259\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5260\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5261\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5262\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5263\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5264\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5265\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5266\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ds1/lib/python3.11/site-packages/pandas/core/generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds1/lib/python3.11/site-packages/pandas/core/generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ds1/lib/python3.11/site-packages/pandas/core/indexes/base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['index'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Drop date columns for the correlation matrix\n",
    "columns_drop = ['an', 'mois', 'jour', 'index']\n",
    "df_select = df_encoded.drop(columns_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79448e-c276-43b9-a87e-9d2ea2157253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "df_numeric_scaled = pd.DataFrame(scaler.fit_transform(df_select), columns=df_select.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88a9ea-8230-4d0e-ae55-77a749dfbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "correlation_matrix = df_numeric_scaled.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Generate the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "# Display the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712a786-07c6-4f26-94b5-640f756e2433",
   "metadata": {},
   "source": [
    "The correlation matrix shows some intervariable correlations, but no real importance of variables to predict the target `fatal`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19180e-4918-4d1f-922a-14525d1a1c29",
   "metadata": {},
   "source": [
    "### Preparation of the training and test datasets\n",
    "#### Split data\n",
    "The dataset is split by year slides. The years 2005 to 2019 are selected for the train dataset and the years 2020 and 2021 for the test dataset.\n",
    "\n",
    "~In addition, the test dataset is split to a train and evaluation dataset with an evaluation set of 20% of the testing data.~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2bc919-2233-4813-840b-b7b0bd7b582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to train and test data\n",
    "X_train = df_encoded.loc[df_encoded['an'] < 2020].drop('fatal', axis = 1)\n",
    "X_test = df_encoded.loc[df_encoded['an'] > 2019].drop('fatal', axis = 1)\n",
    "\n",
    "y_train = df_encoded['fatal'].loc[df_encoded['an'] < 2020]\n",
    "y_test = df_encoded['fatal'].loc[df_encoded['an'] > 2019]\n",
    "\n",
    "#split the train dataset to train and eval data\n",
    "#X_test, X_eval, y_test, y_eval = train_test_split(X_train, y_train, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26625aa3-4aa4-407e-af35-fc4f79e5d944",
   "metadata": {},
   "source": [
    "#### Data scaling\n",
    "Decision trees are not sensitive to different scales and thus scaling is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0cee2-929f-4711-8370-63b1a875f475",
   "metadata": {},
   "source": [
    "#### Create xgboost matrices\n",
    "xgboost requires a specific matrix format. Depends on the training method."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6da13135-86b0-41fb-a762-15da43fdd821",
   "metadata": {},
   "source": [
    "# Create the xgb matrices\n",
    "train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "#eval = xgb.DMatrix(data=X_eval, label=y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935884d9-2884-4e18-a40e-2f6d6d4c6a93",
   "metadata": {},
   "source": [
    "### Modelling the data using a XGBoost Classification Model with Tree-based Parzen Estimators optimisation\n",
    "The Tree-based Parzen Estimator (TPE) [1] optimisation combines a Bayesian Sequential Model Based Optimisation (SMBO) and a random search on the hyperparemeter grid[2]. While the random search is a static approach, the SMBO optimises the model using prior runs to determine future points of exploration.\n",
    "\n",
    "The package `hyperopt` [4] is used for optimisation. The hyperparameter optimisation code is written based on ideas in [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899c3a5-71a0-4cc1-8b15-b8f860f89a90",
   "metadata": {},
   "source": [
    "#### The tuning space\n",
    "`hyperopt` uses a specific format of parameters expressions [7]. Hints on the parameter ranges are given in [1,5,7].\n",
    "\n",
    "The `booster` in this first model is `gbtree`. The evaluation metric is `logloss` or `auc`[8].\n",
    "\n",
    "In a later state, also Dropouts meet Multiple Additive Regression Trees (`dart`)[9] is tested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2c349-31d2-4b9e-9573-1e69ce36b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tuning space\n",
    "booster = 'gbtree'\n",
    "eval_metric = 'auc'\n",
    "\n",
    "tuning_space={\n",
    "    'eta': hp.uniform('eta', 0, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'gamma': hp.uniform ('gamma', 0, 10),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha', 0, 100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda', 0, 10),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0, 1),\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0, 1),\n",
    "    'colsample_bynode' : hp.uniform('colsample_bynode', 0, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 1000, 1),\n",
    "    'seed': 123,\n",
    "    'early_stopping_rounds': 10,\n",
    "    'objective': 'binary:logistic',\n",
    "    'subsample': hp.uniform('subsample', 0, 1)\n",
    "    }\n",
    "\n",
    "if (eval_metric == 'auc') or (eval_metric == 'logloss'):\n",
    "    tuning_space.update({'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 2)}) # helps in auc\n",
    "    tuning_space.update({'max_delta_step': hp.uniform('max_delta_step', 0, 2)}) # helps in auc\n",
    "    tuning_space.update({'eval_metric': eval_metric})\n",
    "    tuning_space.update({'booster': booster})\n",
    "\n",
    "if (booster == 'dart'):\n",
    "    tuning_space.update({'rate_drop': hp.uniform('rate_drop', 0, 1)})\n",
    "    tuning_space.update({'one_drop': 0})\n",
    "    tuning_space.update({'skip_drop': hp.uniform('skip_drop', 0, 1)})\n",
    "\n",
    "tuning_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b7c6d-e438-43cc-8ec7-2adb7f2e57aa",
   "metadata": {},
   "source": [
    "#### The tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144da29f-81b6-4bb8-861c-b990c9c87a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the classification\n",
    "def hp_xgbclass(space, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):\n",
    "    \"\"\" Function for Bayesian hyperparameter optimisation of XGboost classification model.\n",
    "    Accepts a parameter `space` and the training and testing data set.\n",
    "        :param tuning_space: a catalog containing the parameters used for hyperparameter tuning, ranges given in hp objects\n",
    "        :param X_train: accepts the train data\n",
    "        :param X_test: accepts the test data\n",
    "        :param X_eval: accepts the eval data used for optimisation\n",
    "        :param y_train: accepts the train target\n",
    "        :param y_test: accepts the test target\n",
    "        :param y_eval: accepts the eval data used for optimisation\n",
    "    \"\"\"\n",
    "\n",
    "    if (space['objective'] == 'binary:logistic') and (space['booster'] == 'gbtree'):\n",
    "        clf=xgb.XGBClassifier(\n",
    "            eta = space['eta'],\n",
    "            eval_metric = space['eval_metric'],\n",
    "            booster = space['booster'],\n",
    "            n_estimators = np.int64(space['n_estimators']),\n",
    "            max_depth = np.int64(space['max_depth']),\n",
    "            gamma = space['gamma'],\n",
    "            reg_alpha = space['reg_alpha'],\n",
    "            min_child_weight=np.int64(space['min_child_weight']),\n",
    "            colsample_bytree=space['colsample_bytree'],\n",
    "            colsample_bylevel=space['colsample_bylevel'],\n",
    "            colsample_bynode=space['colsample_bynode'],\n",
    "            early_stopping_rounds = space['early_stopping_rounds'],\n",
    "            objective = space['objective'],\n",
    "            scale_pos_weight = space['scale_pos_weight'],\n",
    "            max_delta_step = space['max_delta_step'],\n",
    "            subsample = space['subsample']\n",
    "        )\n",
    "            \n",
    "\n",
    "    if (space['objective'] == 'binary:logistic') and (space['booster'] == 'dart'):\n",
    "        clf=xgb.XGBClassifier(\n",
    "            eta = space['eta'],\n",
    "            eval_metric = space['eval_metric'],\n",
    "            booster = space['booster'],\n",
    "            n_estimators = np.int64(space['n_estimators']),\n",
    "            max_depth = np.int64(space['max_depth']),\n",
    "            gamma = space['gamma'],\n",
    "            reg_alpha = space['reg_alpha'],\n",
    "            min_child_weight=np.int64(space['min_child_weight']),\n",
    "            colsample_bytree=space['colsample_bytree'],\n",
    "            colsample_bylevel=space['colsample_bylevel'],\n",
    "            colsample_bynode=space['colsample_bynode'],\n",
    "            early_stopping_rounds = space['early_stopping_rounds'],\n",
    "            objective = space['objective'],\n",
    "            scale_pos_weight = space['scale_pos_weight'],\n",
    "            max_delta_step = space['max_delta_step'],\n",
    "            subsample = space['subsample'],\n",
    "            rate_drop = space['rate_drop'],\n",
    "            one_drop = space['one_drop'],\n",
    "            skip_drop = space['skip_drop']\n",
    "        )\n",
    "    \n",
    "\n",
    "    clf.fit(X_train,\n",
    "            y_train,\n",
    "            eval_set = [(X_test, y_test)],\n",
    "            verbose=False)\n",
    "        \n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred > 0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c54e61-8338-450f-9d02-8e932ed27244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the crossvalidation or load stored parameters\n",
    "\n",
    "# Rerun or get stored parameters\n",
    "cv_rerun = True\n",
    "\n",
    "# define the stored files\n",
    "# gbtree\n",
    "if (tuning_space['eval_metric'] == 'auc') and (tuning_space['booster'] == 'gbtree'):\n",
    "    out_file_trial = './data/XGboost_model/160923_XGboost_gbtree_auc_trial_db'\n",
    "    out_file_params = './data/XGboost_model/160923_XGboost_gbtree_auc_hyperparameters'\n",
    "    \n",
    "elif (tuning_space['eval_metric'] == 'lossless') and (tuning_space['booster'] == 'gbtree'):\n",
    "    out_file_trial = './data/XGboost_model/160923_XGboost_gbtree_lossless_trial_db'\n",
    "    out_file_params = './data/XGboost_model/160923_XGboost_gbtree_lossles_hyperparameters'\n",
    "\n",
    "# dart\n",
    "elif (tuning_space['eval_metric'] == 'auc') and (tuning_space['booster'] == 'dart'):\n",
    "    out_file_trial = './data/XGboost_model/160923_XGboost_dart_auc_trial_db'\n",
    "    out_file_params = './data/XGboost_model/160923_XGboost_dart_auc_hyperparameters'\n",
    "\n",
    "elif (tuning_space['eval_metric'] == 'lossless') and (tuning_space['booster'] == 'dart'):\n",
    "    out_file_trial = './data/XGboost_model/160923_XGboost_dart_lossless_trial_db'\n",
    "    out_file_params = './data/XGboost_model/160923_XGboost_dart_lossles_hyperparameters'\n",
    "\n",
    "\n",
    "# rerun the crossvalidation\n",
    "if (cv_rerun == True):\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(fn = hp_xgbclass,\n",
    "                       space = tuning_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = 100,\n",
    "                       trials = trials)\n",
    "    # save the best parameters and the trials database\n",
    "    with open(out_file_trial, 'wb') as f:\n",
    "        pickle.dump(trials, f)\n",
    "\n",
    "    with open(out_file_params, 'wb') as f:\n",
    "        pickle.dump(best_hyperparams, f)\n",
    "\n",
    "# get the stored hyperparameters\n",
    "else:\n",
    "    with open('./data/XGboost_model/160923_XGboost_gbtree_auc_hyperparameters', 'rb') as f:\n",
    "        best_hyperparams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8684917-11ae-4664-819c-2d87f36c34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05242bdd-d2ca-4447-bf23-ec396d4b99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_space['booster'] == \"gbtree\"):\n",
    "     xgb_1 = xgb.XGBClassifier(\n",
    "        booster = tuning_space['booster'],\n",
    "        eta = best_hyperparams['eta'],\n",
    "        gamma = best_hyperparams['gamma'],\n",
    "        max_depth = np.int64(best_hyperparams['max_depth']),\n",
    "        min_child_weight = np.int64(best_hyperparams['min_child_weight']),\n",
    "        reg_alpha = best_hyperparams['reg_alpha'],\n",
    "        reg_lambda = best_hyperparams['reg_lambda'],\n",
    "        eval_metric = tuning_space['eval_metric'],\n",
    "        objective = tuning_space['objective'],\n",
    "        seed = tuning_space['seed'],\n",
    "        early_stopping_rounds = tuning_space['early_stopping_rounds'],\n",
    "        n_estimators = np.int64(best_hyperparams['n_estimators']),\n",
    "        subsample = best_hyperparams['subsample'],\n",
    "        scale_pos_weight = best_hyperparams['scale_pos_weight'],\n",
    "        max_delta_step = best_hyperparams['max_delta_step'],\n",
    "        colsample_bylevel = best_hyperparams['colsample_bylevel'],\n",
    "        colsample_bynode = best_hyperparams['colsample_bynode'],\n",
    "        colsample_bytree = best_hyperparams['colsample_bytree']\n",
    "    )\n",
    "\n",
    "\n",
    "if (tuning_space['booster'] == \"dart\"):\n",
    "     xgb_1 = xgb.XGBClassifier(\n",
    "        booster = tuning_space['booster'],\n",
    "        eta = best_hyperparams['eta'],\n",
    "        gamma = best_hyperparams['gamma'],\n",
    "        max_depth = np.int64(best_hyperparams['max_depth']),\n",
    "        min_child_weight = np.int64(best_hyperparams['min_child_weight']),\n",
    "        reg_alpha = best_hyperparams['reg_alpha'],\n",
    "        reg_lambda = best_hyperparams['reg_lambda'],\n",
    "        eval_metric = tuning_space['eval_metric'],\n",
    "        objective = tuning_space['objective'],\n",
    "        seed = tuning_space['seed'],\n",
    "        early_stopping_rounds = tuning_space['early_stopping_rounds'],\n",
    "        n_estimators = np.int64(best_hyperparams['n_estimators']),\n",
    "        subsample = best_hyperparams['subsample'],\n",
    "        scale_pos_weight = best_hyperparams['scale_pos_weight'],\n",
    "        max_delta_step = best_hyperparams['max_delta_step'],\n",
    "        colsample_bylevel = best_hyperparams['colsample_bylevel'],\n",
    "        colsample_bynode = best_hyperparams['colsample_bynode'],\n",
    "        colsample_bytree = best_hyperparams['colsample_bytree'],\n",
    "        rate_drop = best_hyperparams['rate_drop'],\n",
    "        one_drop = space['one_drop'],\n",
    "        skip_drop = best_hyperparams['skip_drop']\n",
    "    )\n",
    "  \n",
    "xgb_1 = xgb_1.fit(X_train,\n",
    "          y_train,\n",
    "          eval_set = [(X_test, y_test)], \n",
    "          verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78936a52-21c8-443a-80b6-a19289ecccc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance_types = ['total_gain', 'gain', 'weight', 'cover', 'total_cover']\n",
    "\n",
    "for f in importance_types:\n",
    "    xgb.plot_importance(xgb_1, max_num_features=50, importance_type=f, title=f)\n",
    "    plt.figure(figsize = (16, 12))\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9b563-5fcd-4744-adca-129cbf69342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_1.predict(X_test)\n",
    "y_pred = [round(value) for value in y_pred]\n",
    "\n",
    "print(classification_report(y_pred, y_test))\n",
    "\n",
    "print(pd.crosstab(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee44902-9a6d-495b-a3cf-74eb43dc3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize = (5, 4))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ed08f-f878-4116-9552-b48ba57473c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d08253-e292-42c1-ab52-c8aeb085fd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d0548-7d90-48af-9e91-b00229b2826c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03aef1fc-ee62-4552-92ab-7710fe24b9f1",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] [Bergstra, J., Yamins, D., Cox, D. D. (2013) Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. TProc. of the 30th International Conference on Machine Learning (ICML 2013), June 2013, pp. I-115 to I-23.](https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)\n",
    "\n",
    "[2] https://towardsdatascience.com/hyperopt-demystified-3e14006eb6fa\n",
    "\n",
    "[3] https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html\n",
    "\n",
    "[4] https://github.com/hyperopt/hyperopt\n",
    "\n",
    "[5] https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning\n",
    "\n",
    "[6] https://github.com/hyperopt/hyperopt/wiki/FMin\n",
    "\n",
    "[7] https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
    "\n",
    "[8] https://www.datamachines.io/blog/auc-vs-log-loss\n",
    "\n",
    "[9] [Rashmi, K.V., Gilad-Bachrach (2015) DART: Dropouts meet Multiple Additive Regression Trees. arXiv:1505.01866v](https://doi.org/10.48550/arXiv.1505.01866)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
